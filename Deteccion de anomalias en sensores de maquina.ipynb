{"cells":[{"cell_type":"markdown","metadata":{},"source":["# one-year-industrial-component-degradation\n","Alumno: Ibarra Medrano Jose Pablo\n","Universidad: UNISON\n","Dataset:  https://www.kaggle.com/inIT-OWL/one-year-industrial-component-degradation"]},{"cell_type":"markdown","metadata":{},"source":["**Importamos las librerias necesarias**"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T06:16:49.991917Z","iopub.status.busy":"2021-11-22T06:16:49.991465Z","iopub.status.idle":"2021-11-22T06:16:58.269307Z","shell.execute_reply":"2021-11-22T06:16:58.268262Z","shell.execute_reply.started":"2021-11-22T06:16:49.991848Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import os\n","import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","from sklearn.cluster import KMeans\n","from sklearn import preprocessing\n","from sklearn.svm import OneClassSVM\n","from numpy.random import seed\n","from keras.layers import Input, Dropout\n","from keras.layers.core import Dense \n","from keras.models import Model, Sequential, load_model\n","from keras import regularizers\n","from keras.models import model_from_json\n","from scipy.special import softmax"]},{"cell_type":"markdown","metadata":{},"source":["# Contexto\n","##### Este dataset contiene la informacion de un compomente en degradacion durante 12 meses y se registraron sus cambios. Y se inició en el proyecto europeo de investigación e innovación IMPROVE."]},{"cell_type":"markdown","metadata":{},"source":["**Primero pasamos el df a un variable llamada main_df el cual contienes todos los archivos de los 12 meses guardados en un solo csv para manejar mas facilmente los datos**"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T06:16:58.271784Z","iopub.status.busy":"2021-11-22T06:16:58.271433Z","iopub.status.idle":"2021-11-22T06:17:02.164596Z","shell.execute_reply":"2021-11-22T06:17:02.163374Z","shell.execute_reply.started":"2021-11-22T06:16:58.271724Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["main_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\n","main_df.describe()"]},{"cell_type":"markdown","metadata":{},"source":["**Ahora busquemos la correlación entre los datos. Como se puede ver, lo único que parece correlacionase es el motor torque, la velocidad de la hoja y el error de retraso de la hoja, también la velocidad VAX y la velocidad de la envolvedora. Excluyendo, por supuesto, el mes y el número de muestra, y todas las autocorrelaciones en la diagonal.**"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T06:17:02.166779Z","iopub.status.busy":"2021-11-22T06:17:02.166398Z","iopub.status.idle":"2021-11-22T06:17:03.419375Z","shell.execute_reply":"2021-11-22T06:17:03.418089Z","shell.execute_reply.started":"2021-11-22T06:17:02.166706Z"},"trusted":true},"outputs":[],"source":["#heatmap e correlaciones de -1 a 1\n","sns.heatmap(main_df.corr(), vmin= -1, vmax = 1)"]},{"cell_type":"markdown","metadata":{},"source":["**Eliminemos algunas columnas y volteemos nuestros valores de columna de motor torque multiplicándo por -1. Todo esto para una mayor comprensión visual de la grafica**"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T06:17:03.421951Z","iopub.status.busy":"2021-11-22T06:17:03.421286Z","iopub.status.idle":"2021-11-22T06:17:06.392027Z","shell.execute_reply":"2021-11-22T06:17:06.390896Z","shell.execute_reply.started":"2021-11-22T06:17:03.421885Z"},"trusted":true},"outputs":[],"source":["main_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\n","#Eliminando columnas sin sentido para esta propuesta                           (axis=1) = columns\n","main_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis=1)\n","#Volteando los valores de la columna\n","main_df['pCut::Motor_Torque'] = main_df['pCut::Motor_Torque'] *-1\n","#Heatmap\n","sns.heatmap(main_df.corr(), vmin= -1, vmax = 1)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Tratamiento de datos no numéricos\n","\n","**En nuestro conjunto de datos, los modos de la máquina pueden influir en los patrones de datos, por lo que necesitamos transformar esos datos de cadenas a clases numéricas. Esta función hará precisamente eso.** "]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T06:17:06.394482Z","iopub.status.busy":"2021-11-22T06:17:06.393850Z","iopub.status.idle":"2021-11-22T06:17:06.410112Z","shell.execute_reply":"2021-11-22T06:17:06.408106Z","shell.execute_reply.started":"2021-11-22T06:17:06.394419Z"},"trusted":true},"outputs":[],"source":["def handle_non_numeric(df):\n","    # Tomamos los valores de las columnas de nuestro df\n","    columns = df.columns.values\n","    \n","    for column in columns:\n","        \n","        # Diccionario con cada valor numerico con cada texto\n","        text_digit_vals = {}\n","        \n","        # Receibimos texto para convertilo a valor numerico\n","        def convert_to_int (val):\n","            \n","\n","            return text_digit_vals[val]\n","        \n","        # Verificamos que los valores no sean flotantes o enteros\n","        if df[column].dtype !=np.int64 and df[column].dtype != np.float64:\n","            \n","            # Obtenemos los valores de la columna actual\n","            column_contents = df[column].values.tolist()\n","            \n","            # Obtenemos los valores unicos de la columna\n","            unique_elements = set(column_contents)\n","            x=0\n","            \n","            for unique in unique_elements:\n","                \n","                if unique not in text_digit_vals:\n","                    text_digit_vals[unique] = x\n","                    x+=1\n","            \n","            df[column] = list(map(convert_to_int, df[column]))\n","    \n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["# Approach\n","\n","**Se probarán algunos algoritmos para ver si podemos obtener información sobre la máquina. Usaremos OneClass SVM y KMeans con 1 clúster para probar la agrupación. Después de eso, probaremos un Autoencoder para intentar reproducir datos basados en el estado de salud de las máquinas.**\n","\n","**En los 3 casos, tomaremos un segmento de las primeras filas y lo consideraremos como el estado correcto de la máquina, luego lo enviaremos a los algoritmos. Después de eso, le daremos a los algoritmos la totalidad del conjunto de datos y veremos cómo funcionan en el resto de los datos. Las desviaciones, puntuaciones bajas y pérdidas elevadas se considerarán anomalías a estudiar.**"]},{"cell_type":"markdown","metadata":{},"source":["# OneClass SVM approach\n","\n","**OneClass SVM se utiliza para la detección de valores atípicos, intenta encontrar 2 clases en los datos, la clase \"normal\" y los valores atípicos. Usaremos la SVM para intentar encontrar valores atípicos y anomalías.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T06:17:06.416592Z","iopub.status.busy":"2021-11-22T06:17:06.415904Z"},"trusted":true},"outputs":[],"source":["#Tomando todo el conjunto de datos\n","main_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\n","#Eliminando columnas con información no deseada / irrelevante para el algoritmo\n","main_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis=1)\n","#Transformando modos en datos clasificados\n","main_df = handle_non_numeric(main_df)\n","\n","X = main_df\n","\n","#Definiendo el procesamiento para los datos\n","scaler = preprocessing.MinMaxScaler()\n","#Preprocesamiento\n","X = pd.DataFrame(scaler.fit_transform(X), \n","                              columns=X.columns, \n","                              index=X.index)\n","\n","\n","#Escalando\n","X = preprocessing.scale(X)\n","#Obtenemos las primeras 200.000 filas.\n","X_train = X[:200000]\n","\n","\n","#Creación de una SVM OneClass adecuada\n","ocsvm = OneClassSVM(nu=0.25, gamma=0.05)\n","ocsvm.fit(X_train)"]},{"cell_type":"markdown","metadata":{},"source":["**Predecir y clasificar el conjunto de datos en anomalías y no anomalías, luego pasarlo a un marco de datos.**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df=main_df.copy()\n","df['anomaly'] = pd.Series(ocsvm.predict(X))"]},{"cell_type":"markdown","metadata":{},"source":["**Guardando el Dataframe**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Saving Dataframe.\n","df.to_csv('Labled_df.csv')"]},{"cell_type":"markdown","metadata":{},"source":["**Leyendo el dataframe**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Reading into dataframe\n","df = pd.read_csv('../input/created/Labled_df.csv', index_col=0)\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["**Visualizando anomalias**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Obteniendo grupos etiquetados\n","scat_1 = df.groupby('anomaly').get_group(1)\n","scat_0 = df.groupby('anomaly').get_group(-1)\n","\n","# Plot size\n","plt.subplots(figsize=(15,7))\n","\n","# Plot group 1 -labeled, color green, point size 1\n","plt.plot(scat_1.index,scat_1['pCut::Motor_Torque'], 'g.', markersize=1)\n","\n","# Plot group -1 -labeled, color red, point size 1\n","plt.plot(scat_0.index, scat_0['pCut::Motor_Torque'],'r.', markersize=1)\n"]},{"cell_type":"markdown","metadata":{},"source":["**Visualizando puntuaciones para todo el conjunto de datos**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Crear un marco de datos para la puntuación de cada muestra de datos\n","score = pd.DataFrame()\n","#Devolviendo puntajes al df\n","score['score'] = ocsvm.score_samples(X)\n","\n","#Plot size\n","plt.subplots(figsize=(15,7))\n","#Plotting\n","score['score'].plot()\n","#Guardando puntajes dataframe\n","score.to_csv('SVM_Score.csv')"]},{"cell_type":"markdown","metadata":{},"source":["**Media móvil de puntuación invertida**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(15,7))\n","\n","\n","((score['score'].rolling(20000).mean())*-1).plot(ax=ax)"]},{"cell_type":"markdown","metadata":{},"source":["**Scatplot para ver la partitura a través del ruido**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.subplots(figsize=(15,7))\n","plt.plot(score.index, score['score'],'r.', markersize=1)"]},{"cell_type":"markdown","metadata":{},"source":["# KMeans approach\n","\n","**El enfoque de Kmeans hará lo mismo que el OC-SVM**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#------ Preparando funciones para entrenamiento y predicción futura -----\n","main_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\n","main_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis=1)\n","main_df = handle_non_numeric(main_df)\n","X = main_df\n","\n","scaler = preprocessing.MinMaxScaler()\n","\n","X = pd.DataFrame(scaler.fit_transform(X), \n","                              columns=X.columns, \n","                              index=X.index)\n","\n","X = preprocessing.scale(X)\n","#-------------------------------------------------------------------\n","\n","\n","#Porcentaje de los datos que se considerarán condición saludable\n","train_percentage = 0.15\n","#Valor entero para el segmento que se considerará en buen estado\n","train_size = int(len(main_df.index)*train_percentage)\n","#Tomando parte de los datos de entrenamiento\n","X_train = X[:train_size]\n","\n","\n","kmeans = KMeans(n_clusters=1)\n","\n","kmeans.fit(X_train)\n","\n","k_anomaly = main_df.copy()\n","\n","k_anomaly = pd.DataFrame(kmeans.transform(X))\n","\n","k_anomaly.to_csv('KM_Distance.csv')\n","\n","plt.subplots(figsize=(15,7))\n","\n","plt.plot(k_anomaly.index, k_anomaly[0], 'g', markersize=1)"]},{"cell_type":"markdown","metadata":{},"source":["# AutoEncoder approach\n","\n","**AutoEncoders son redes neuronales que expanden y comprimen datos en dimensiones más altas y más bajas, luego intenta recrear los datos. La idea es que el autocodificador comprenderá la relación entre las características y, a partir de eso, recreará los datos exactos que se le dieron.**\n","\n","**Entrenaremos el algoritmo con el estado saludable de la máquina. A medida que intenta reconstruir el resto de los datos como el estado correcto, la pérdida de reconstrucción, la diferencia entre los datos de la máquina pronosticados y los datos de la máquina real, se considerará un estado \"incorrecto\"**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#------------------------- Preparando informacion para el entrenamiento --------------------------- \n","main_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\n","main_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis=1)\n","main_df = handle_non_numeric(main_df)\n","X = main_df\n","\n","scaler = preprocessing.MinMaxScaler()\n","\n","X = pd.DataFrame(scaler.fit_transform(X), \n","                              columns=X.columns, \n","                              index=X.index)\n","\n","\n","\n","X = preprocessing.scale(X)\n","\n","\n","train_percentage = 0.15\n","train_size = int(len(main_df.index)*train_percentage)\n","\n","X_train = X[:train_size]\n","#----------------------------------------------------------------------------------\n","\n","\n","\n","#Semilla para validación y entrenamiento de lotes aleatorios\n","seed(10)\n","act_func = 'elu'\n","model=Sequential()\n","\n","# Primera capa oculta, conectada al vector de entrada X.\n","model.add(Dense(50,activation=act_func,\n","                kernel_initializer='glorot_uniform',\n","                kernel_regularizer=regularizers.l2(0.0),\n","                input_shape=(X_train.shape[1],)\n","               )\n","         )\n","# Segunda capa oculta\n","model.add(Dense(10,activation=act_func,\n","                kernel_initializer='glorot_uniform'))\n","# Tercera capa oculta\n","model.add(Dense(50,activation=act_func,\n","                kernel_initializer='glorot_uniform'))\n","\n","# Capa de entrada\n","model.add(Dense(X_train.shape[1],\n","                kernel_initializer='glorot_uniform'))\n","\n","# Función de pérdida y elección del optimizador\n","model.compile(loss='mse',optimizer='adam')\n","\n","# Modelo de tren para 50 épocas, tamaño de lote de 200\n","NUM_EPOCHS=50\n","BATCH_SIZE=200\n","\n","#Aprovechando la validación y la pérdida de entrenamiento durante épocas\n","history=model.fit(np.array(X_train),np.array(X_train),\n","                  batch_size=BATCH_SIZE, \n","                  epochs=NUM_EPOCHS,\n","                  validation_split=0.1,\n","                  verbose = 1)\n"]},{"cell_type":"markdown","metadata":{},"source":["**Graficar la pérdida de validación y la pérdida de entrenamiento a lo largo de los epochs**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.subplots(figsize=(15,7))\n","\n","plt.plot(history.history['loss'],'b',label='Training loss')\n","plt.plot(history.history['val_loss'],'r',label='Validation loss')\n","plt.legend(loc='upper right')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss, [mse]')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Ahora alimentaremos el algoritmo con los mismos datos de entrenamiento y haremos que intente reconstruir los datos. Luego veremos la distribución de la pérdida sobre los datos de entrenamiento, más adelante usaremos esta distribución para determinar algunos umbrales.**"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["#Reconstrucción de datos de entrenamiento\n","X_pred = model.predict(np.array(X_train))\n","X_pred = pd.DataFrame(X_pred,columns=main_df.columns)\n","X_pred.index = pd.DataFrame(X_train).index\n","\n","scored = pd.DataFrame(index=pd.DataFrame(X_train).index)\n","scored['Loss_mae'] = np.mean(np.abs(X_pred-X_train), axis = 1)\n","\n","plt.subplots(figsize=(15,7))\n","sns.distplot(scored['Loss_mae'],\n","             bins = 15, \n","             kde= True,\n","            color = 'blue');\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["**Ahora para hacer lo mismo pero con todos nuestros datos para ver la pérdida a lo largo del tiempo, esto nos dará datos interesantes.**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","#Reconstruyendo toda la informacion\n","X_pred = model.predict(np.array(X))\n","X_pred = pd.DataFrame(X_pred,columns=main_df.columns)\n","X_pred.index = pd.DataFrame(X).index\n","\n","#Devolviendo la media de las pérdidas para cada columna y poniéndola en un marco de datos\n","scored = pd.DataFrame(index=pd.DataFrame(X).index)\n","scored['Loss_mae'] = np.mean(np.abs(X_pred-X), axis = 1)\n","\n","plt.subplots(figsize=(15,7))\n","\n","\n","#Guardando el dataframe\n","scored.to_csv('AutoEncoder_loss.csv')\n","\n","plt.plot(scored['Loss_mae'],'b',label='Prediction Loss')\n","\n","plt.legend(loc='upper right')\n","plt.xlabel('Sample')\n","plt.ylabel('Loss, [mse]')"]},{"cell_type":"markdown","metadata":{},"source":["# Resultados del analysis"]},{"cell_type":"markdown","metadata":{},"source":["### Diagrama de dispersión para la puntuación de cada algoritmo, para ver a través del ruido."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.subplots(figsize=(15,7))\n","\n","enc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n","plt.plot(enc_loss.index,enc_loss['Loss_mae'], 'g.', markersize=1,label=\"AutoEncoder Loss\")\n","plt.legend(loc='upper right')\n","plt.xlabel('Sample')\n","plt.show()\n","\n","plt.subplots(figsize=(15,7))\n","k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n","plt.plot(k_anomaly.index,k_anomaly['0'], 'g.', markersize=1,label=\"KM cluster Distance\")\n","plt.legend(loc='upper right')\n","plt.xlabel('Sample')\n","plt.show()\n","\n","plt.subplots(figsize=(15,7))\n","score = pd.read_csv('../input/created/SVM_Score.csv')\n","plt.plot(score.index,score['score'], 'g.', markersize=1,label=\"OCSVM score\")\n","plt.legend(loc='upper right')\n","plt.xlabel('Sample')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Graficando cada algoritmo, con OCSVM invertido sobre 0.**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Plot size\n","plt.subplots(figsize=(15,7))\n","\n","#Leyendo cada csv\n","k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n","score = pd.read_csv('../input/created/SVM_Score.csv')\n","enc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n","\n","#Escalando datos para la visualización\n","k_distance = k_anomaly/k_anomaly.max()\n","svm_score = (score/score.max())*-1\n","\n","plt.plot(enc_loss.index,enc_loss['Loss_mae'], label=\"AutoEncoder Loss\")\n","plt.plot(svm_score.index, svm_score['score'],label=\"OCSVM score\")\n","plt.plot(k_distance.index,k_distance['0'], label=\"Kmeans Euclidean Dist\")\n","\n","\n","\n","plt.gca().legend(('AutoEncoder Loss','OCSVM score * -1','Kmeans Euclidean Dist'))\n"]},{"cell_type":"markdown","metadata":{},"source":["**Buscando correlación entre los algoritmos**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n","score = pd.read_csv('../input/created/SVM_Score.csv')\n","enc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n","\n","corr = pd.DataFrame()\n","corr['SVM_score'] = score['score']\n","corr['KM_cluster_distance'] = k_anomaly['0']\n","corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n","corr.corr()"]},{"cell_type":"markdown","metadata":{},"source":["**Comparando los tres metodos de deteccion de anomalia por medio de Scatter plots con media móvil**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#---- Leyendo datos y pasarlos al marco de datos nuevamente ----- \n","\n","k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n","score = pd.read_csv('../input/created/SVM_Score.csv')\n","enc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n","\n","corr = pd.DataFrame()\n","corr['SVM_score'] = score['score']\n","corr['KM_cluster_distance'] = k_anomaly['0']\n","corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n","#---------------------------------------------------------\n","\n","\n","\n","plt.subplots(figsize=(15,7))\n","\n","#Scatter plot de SVM\n","plt.plot(corr.index, corr['SVM_score'], 'g.', markersize=1, label = 'OCSVM_score')\n","#Trazado de la media móvil de 1000\n","plt.plot(corr.index, corr['SVM_score'].rolling(1000).mean(), 'r', markersize=1, label = 'Moving Mean')\n","plt.legend(loc='upper right')\n","plt.show()\n","\n","\n","plt.subplots(figsize=(15,7))\n","plt.plot(corr.index, corr['KM_cluster_distance'], 'g.', markersize=1, label = 'KM_cluster_distance')\n","plt.plot(corr.index, corr['KM_cluster_distance'].rolling(1000).mean(), 'r', markersize=1, label = 'Moving Mean')\n","plt.legend(loc='upper right')\n","plt.show()\n","\n","\n","plt.subplots(figsize=(15,7))\n","plt.plot(corr.index, corr['AutoEnc_loss'], 'g.', markersize=1, label = 'AutoEnc_loss')\n","plt.plot(corr.index, corr['AutoEnc_loss'].rolling(1000).mean(), 'r', markersize=1, label = 'Moving Mean')\n","plt.legend(loc='upper right')\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["**Distribución de pérdidas sobre los datos de entrenamiento**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n","score = pd.read_csv('../input/created/SVM_Score.csv')\n","enc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n","\n","corr = pd.DataFrame()\n","corr['SVM_score'] = score['score']\n","corr['KM_cluster_distance'] = k_anomaly['0']\n","corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n","\n","\n","\n","plt.subplots(figsize=(10,7))\n","sns.distplot(corr['SVM_score'].head(160000), bins=15)\n","plt.show()\n","\n","plt.subplots(figsize=(10,7))\n","sns.distplot(corr['KM_cluster_distance'].head(160000),bins=15)\n","plt.show()\n","\n","plt.subplots(figsize=(10,7))\n","sns.distplot(corr['AutoEnc_loss'].head(160000),bins=15)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Distribución de pérdidas en todo el conjunto de datos**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n","score = pd.read_csv('../input/created/SVM_Score.csv')\n","enc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n","\n","corr = pd.DataFrame()\n","corr['SVM_score'] = score['score']\n","corr['KM_cluster_distance'] = k_anomaly['0']\n","corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n","\n","plt.subplots(figsize=(10,7))\n","sns.distplot(corr['SVM_score'], bins=15)\n","plt.show()\n","\n","plt.subplots(figsize=(10,7))\n","sns.distplot(corr['KM_cluster_distance'],bins=15)\n","plt.show()\n","\n","plt.subplots(figsize=(10,7))\n","sns.distplot(corr['AutoEnc_loss'],bins=15)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Ahora usaremos la información sobre la distribución de la pérdida de entrenamiento para determinar algunos umbrales para los gráficos. También se trazarán los medios móviles.**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n","score = pd.read_csv('../input/created/SVM_Score.csv')\n","enc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n","\n","corr = pd.DataFrame()\n","corr['SVM_score'] = score['score']\n","corr['KM_cluster_distance'] = k_anomaly['0']\n","corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n","\n","\n","\n","#Creando una matriz para que los umbrales se tracen en todo el conjunto de datos\n","lower_threshold = np.full((corr['SVM_score'].size, 1), 0)\n","upper_threshold = np.full((corr['SVM_score'].size, 1), 18000)\n","high_density_threshold = np.full((corr['SVM_score'].size, 1), 13250)\n","\n","plt.subplots(figsize=(15,7))\n","plt.plot(corr.index, corr['SVM_score'], 'k', markersize=1, label = 'OCSVM_score')\n","plt.plot(corr.index, corr['SVM_score'].rolling(100).mean(), 'r', markersize=1, label = 'Moving Mean')\n","plt.plot(corr.index, lower_threshold, label='Lower Threshold')\n","plt.plot(corr.index, upper_threshold, label = 'Upper Threshold')\n","plt.plot(corr.index, high_density_threshold, label = 'Highest Density')\n","plt.legend(loc='upper right')\n","plt.show()\n","\n","\n","lower_threshold = np.full((corr['KM_cluster_distance'].size, 1), 1.2)\n","upper_threshold = np.full((corr['KM_cluster_distance'].size, 1), 17.5)\n","high_density_threshold = np.full((corr['KM_cluster_distance'].size, 1), 2.5)\n","\n","plt.subplots(figsize=(15,7))\n","  \n","plt.plot(corr.index, corr['KM_cluster_distance'], 'k', markersize=1, label = 'KM_cluster_distance')\n","plt.plot(corr.index, corr['KM_cluster_distance'].rolling(100).mean(), 'r', markersize=1, label = 'Moving Mean')\n","plt.plot(corr.index, lower_threshold, label='Lower Threshold')\n","plt.plot(corr.index, upper_threshold, label = 'Upper Threshold')\n","plt.plot(corr.index, high_density_threshold, label = 'Highest Density')\n","plt.legend(loc='upper right')\n","plt.show()\n","\n","lower_threshold = np.full((corr['AutoEnc_loss'].size, 1), 0)\n","upper_threshold = np.full((corr['AutoEnc_loss'].size, 1), 0.1)\n","high_density_threshold = np.full((corr['AutoEnc_loss'].size, 1), 0.05)\n","\n","plt.subplots(figsize=(15,7))\n","  \n","plt.plot(corr.index, corr['AutoEnc_loss'], 'k', markersize=1, label = 'AutoEnc_loss')\n","plt.plot(corr.index, corr['AutoEnc_loss'].rolling(100).mean(), 'r', markersize=1, label = 'Moving Mean')\n","plt.plot(corr.index, lower_threshold, label='Lower Threshold')\n","plt.plot(corr.index, upper_threshold, label = 'Upper Threshold')\n","plt.plot(corr.index, high_density_threshold, label = 'Highest Density')\n","plt.legend(loc='upper right')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Scatter plot de los puntajes del algoritmo vs los otros algoritmos**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n","score = pd.read_csv('../input/created/SVM_Score.csv')\n","enc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n","\n","corr = pd.DataFrame()\n","corr['SVM_score'] = score['score']\n","corr['KM_cluster_distance'] = k_anomaly['0']\n","corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n","\n","plt.subplots(figsize=(15,7))\n","  \n","plt.plot(corr['KM_cluster_distance'],corr['SVM_score'],'b.',markersize=1 )\n","plt.xlabel('KM')\n","plt.ylabel('SVM')\n","plt.show()\n","\n","plt.subplots(figsize=(15,7))\n","  \n","plt.plot(corr['AutoEnc_loss'],corr['SVM_score'],'b.' ,markersize=1 )\n","plt.xlabel('Encoder')\n","plt.ylabel('SVM')\n","plt.show()\n","\n","plt.subplots(figsize=(15,7))\n","  \n","plt.plot(corr['AutoEnc_loss'],corr['KM_cluster_distance'],'b.' ,markersize=1 )\n","plt.xlabel('Encoder')\n","plt.ylabel('KM')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Continuing with Autoencoder\n","**Aunque existe cierta simularidad de anomalías visuales entre los algoritmos, los algoritmos de agrupamiento nos dan mucho ruido y poco en lo que trabajar. Por otro lado, el codificador automático tiene una carrera casi segura hasta el punto de falla. No podemos concluir con absoluta certeza que los componentes hayan cambiado después del pico de pérdida más alto, pero es muy posible.**\n","\n","**Además, ahora analizaremos la pérdida del codificador por mes, con los umbrales.**  "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n","score = pd.read_csv('../input/created/SVM_Score.csv')\n","enc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n","\n","corr = pd.DataFrame()\n","corr['SVM_score'] = score['score']\n","corr['KM_cluster_distance'] = k_anomaly['0']\n","corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n","\n","main_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\n","\n","#Pasar la pérdida del codificador al marco de datos principal, para que sea más fácil separar por mes\n","main_df['AutoEnc_loss'] = corr['AutoEnc_loss']\n","\n","#Obteniendo lista de los meses\n","months = main_df['month'].dropna().unique()\n","\n","#Recorriendo cada mes\n","for month in months:\n","    #Tomando la porción del marco de datos para cada mes\n","    month_df = main_df.groupby('month').get_group(month)\n","    \n","    upper_threshold = np.full((month_df['AutoEnc_loss'].size, 1), 0.1)\n","    high_density_threshold = np.full((month_df['AutoEnc_loss'].size, 1), 0.05)\n","    \n","    plt.subplots(figsize=(15,7))\n","    plt.plot(month_df.index, month_df['AutoEnc_loss'], label=f'AutoEnc_loss month_{month}')\n","    plt.plot(month_df.index, upper_threshold, label = 'Upper Threshold')\n","    plt.plot(month_df.index, high_density_threshold, label = 'Highest Density')\n","    plt.legend(loc='upper right')\n","    plt.ylim(0,1.3)\n","    \n","    plt.show()\n","    \n","    "]},{"cell_type":"markdown","metadata":{},"source":["**Ahora veremos la distribución de pérdidas por mes.**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n","score = pd.read_csv('../input/created/SVM_Score.csv')\n","enc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n","\n","corr = pd.DataFrame()\n","corr['SVM_score'] = score['score']\n","corr['KM_cluster_distance'] = k_anomaly['0']\n","corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n","\n","main_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\n","\n","main_df['AutoEnc_loss'] = corr['AutoEnc_loss']\n","\n","months = main_df['month'].dropna().unique()\n","\n","for month in months:\n","    month_df = main_df.groupby('month').get_group(month)    \n","    \n","    plt.subplots(figsize=(15,7))\n","    sns.distplot((month_df['AutoEnc_loss']), bins=15).set_title(f'Month {month} Loss Distribution')\n","    #X axis limits\n","    plt.xlim([-1.2,1.2])\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Una forma interesante de ver la anomalía es la cola de la distribución. Como pudimos ver en la gráfica por mes, el cuarto mes tiene el punto de anomalía más alto. También es evidente la influencia de eso en la distribución de la pérdida. Quizás al observar la curtosis de cada mes podamos obtener más información.**\n","\n","**La curtosis nos informará sobre la forma de la distribución. Una curtosis alta significa que muchos puntos de datos tienen el mismo valor y que las colas o la desviación estándar son realmente pequeñas o inexistentes (en nuestro caso, muchos puntos de datos cercanos a 0 significan una buena condición de la máquina). Curtosis baja significa que tenemos muchos puntos de datos dispersos, lo que le da a la distribución colas anchas y gruesas, casi del tamaño de su pico.** "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n","score = pd.read_csv('../input/created/SVM_Score.csv')\n","enc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n","\n","corr = pd.DataFrame()\n","corr['SVM_score'] = score['score']\n","corr['KM_cluster_distance'] = k_anomaly['0']\n","corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n","\n","main_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\n","\n","main_df['AutoEnc_loss'] = corr['AutoEnc_loss']\n","\n","months = main_df['month'].dropna().unique()\n","\n","for month in months:\n","    month_df = main_df.groupby('month').get_group(month)\n","    kurt = (month_df['AutoEnc_loss']).kurtosis()\n","    print(f'Month {month} kurtosis = {kurt}')"]},{"cell_type":"markdown","metadata":{},"source":["**Así que los meses con baja curtosis son los meses con más anomalías, lo que nos puede decir un poco sobre el estado de la máquina.**"]},{"cell_type":"markdown","metadata":{},"source":["# Detección de sensor\n","\n","**Ahora que sabemos dónde tiene un problema la máquina, intentaremos encontrar qué componente / sensor está causando esta perturbación en el codificador automático. Para eso lo entrenaremos nuevamente, y obtendremos sus predicciones y pérdidas para cada columna para ver cuál tiene la mayor contribución a la pérdida total.**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["main_df = pd.read_csv('../input/datasetsone-year-compiledcsv/One_year_compiled.csv')\n","main_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis=1)\n","main_df = handle_non_numeric(main_df)\n","X = main_df\n","\n","scaler = preprocessing.MinMaxScaler()\n","\n","X = pd.DataFrame(scaler.fit_transform(X), \n","                              columns=X.columns, \n","                              index=X.index)\n","X = preprocessing.scale(X)\n","\n","train_percentage = 0.15\n","train_size = int(len(main_df.index)*train_percentage)\n","\n","X_train = X[:train_size]\n","\n","seed(10)\n","\n","act_func = 'elu'\n","\n","# Input layer:\n","model=Sequential()\n","# First hidden layer, connected to input vector X. \n","model.add(Dense(50,activation=act_func,\n","                kernel_initializer='glorot_uniform',\n","                kernel_regularizer=regularizers.l2(0.0),\n","                input_shape=(X_train.shape[1],)\n","               )\n","         )\n","\n","model.add(Dense(10,activation=act_func,\n","                kernel_initializer='glorot_uniform'))\n","\n","model.add(Dense(50,activation=act_func,\n","                kernel_initializer='glorot_uniform'))\n","\n","model.add(Dense(X_train.shape[1],\n","                kernel_initializer='glorot_uniform'))\n","\n","model.compile(loss='mse',optimizer='adam')\n","\n","# Train model for 100 epochs, batch size of 10: \n","NUM_EPOCHS=50\n","BATCH_SIZE=200\n","\n","history=model.fit(np.array(X_train),np.array(X_train),\n","                  batch_size=BATCH_SIZE, \n","                  epochs=NUM_EPOCHS,\n","                  validation_split=0.1,\n","                  verbose = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Prediciendo y pasando la predicción al dataframe\n","X_pred = model.predict(np.array(X))\n","X_pred = pd.DataFrame(X_pred,columns=main_df.columns)\n","X_pred.index = pd.DataFrame(main_df).index\n","\n","#Pasando X de un array a un dataframe\n","X = pd.DataFrame(X,columns=main_df.columns)\n","X.index = pd.DataFrame(main_df).index\n","\n","#Dataframe donde irá toda la pérdida por columnas\n","loss_df = pd.DataFrame()\n","\n","main_df.drop('mode',axis=1, inplace=True)\n","\n","for column in main_df.columns:\n","    loss_df[f'{column}'] = (X_pred[f'{column}'] - X[f'{column}']).abs()\n","     \n","    plt.subplots(figsize=(15,7))\n","    plt.plot(loss_df.index, loss_df[f'{column}'], label=f'{column} loss')\n","    plt.legend(loc='upper right')\n","    \n","    plt.show()\n","\n","loss_df.to_csv('AutoEncoder_loss_p_column.csv')"]},{"cell_type":"markdown","metadata":{},"source":["**Ahora aplicaremos la función Softmax a cada fila para que podamos obtener el porcentaje que cada columna contribuye a la pérdida total. En cuanto a la suma de cada fila nos dará 1.**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sftmax_df = pd.read_csv('../input/created/AutoEncoder_loss_p_column.csv', index_col=0)\n","sftmax_df = softmax(sftmax_df, axis=1)\n","sftmax_df.describe()"]},{"cell_type":"markdown","metadata":{},"source":["**Trazando el porcentaje de la contribución de cada columna a la pérdida total**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for column in sftmax_df.columns:\n","    \n","\n","    plt.subplots(figsize=(15,7))\n","    plt.plot(sftmax_df.index, sftmax_df[f'{column}'], label=f'{column} loss')\n","    plt.legend(loc='upper right')\n","    \n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Ahora podemos trazar un diagrama de pila para visualizar mejor la contribución de cada columna a la pérdida total. Como verá, la posición de Blades contribuye mucho a la pérdida total en ese pico que vimos. Miraremos más de cerca a esa porción.**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.subplots(figsize=(15,7))\n","\n","#Labels for stackbar plot\n","df_label = ['Torque', 'Cut lag','Cut speed','Cut position','Film position','Film speed','Film lag','VAX']\n","\n","#Stackbar plot\n","plt.stackplot(sftmax_df.index, sftmax_df['pCut::Motor_Torque'],\n","             sftmax_df['pCut::CTRL_Position_controller::Lag_error'],\n","             sftmax_df['pCut::CTRL_Position_controller::Actual_speed'],\n","              sftmax_df['pCut::CTRL_Position_controller::Actual_position'],\n","             sftmax_df['pSvolFilm::CTRL_Position_controller::Actual_position'],\n","             sftmax_df['pSvolFilm::CTRL_Position_controller::Actual_speed'],\n","             sftmax_df['pSvolFilm::CTRL_Position_controller::Lag_error'],\n","             sftmax_df['pSpintor::VAX_speed'],\n","             labels = df_label)\n","\n","plt.legend(loc='upper center', ncol=8)\n","\n","plt.ylim(0,1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.subplots(figsize=(15,7))\n","\n","df_label = ['Torque', 'Cut lag','Cut speed','Cut position','Film position','Film speed','Film lag','VAX']\n","\n","sftmax_df = sftmax_df[400000:600000]\n","\n","plt.stackplot(sftmax_df.index, sftmax_df['pCut::Motor_Torque'],\n","             sftmax_df['pCut::CTRL_Position_controller::Lag_error'],\n","             sftmax_df['pCut::CTRL_Position_controller::Actual_speed'],\n","              sftmax_df['pCut::CTRL_Position_controller::Actual_position'],\n","             sftmax_df['pSvolFilm::CTRL_Position_controller::Actual_position'],\n","             sftmax_df['pSvolFilm::CTRL_Position_controller::Actual_speed'],\n","             sftmax_df['pSvolFilm::CTRL_Position_controller::Lag_error'],\n","             sftmax_df['pSpintor::VAX_speed'],\n","             labels = df_label)\n","\n","plt.legend(loc='upper center', ncol=8)\n","\n","plt.ylim(0,1)"]},{"cell_type":"markdown","metadata":{},"source":["**Parece que el error de retraso de la hoja también proporciona una gran parte de la pérdida total. La posible explicación aquí es que la cuchilla está desgastada y, por eso, está comenzando a desviarse del camino que la máquina intenta trazar para la cuchilla al cortar la película.**\n","\n","**Ahora veremos la distribución de la contribución de cada columna a la pérdida total, solo para comprender mejor qué sensores están dando mayor pérdida.**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for column in sftmax_df.columns:\n","    \n","\n","    plt.subplots(figsize=(15,7))\n","    \n","    sns.distplot(( sftmax_df[f'{column}']), bins=15).set_title(f'Contribution Distribution')\n","    plt.xlim(0,1)\n","    \n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Conclusion \n","\n","**Con toda la información recopilada, podríamos decir dónde y cuándo la máquina sufrió una degradación masiva. También pudimos decir cuál de las medidas contribuye más a la pérdida en todo el año de la máquina, lo que nos dice que estos componentes fabricados necesitan más atención. Se pueden utilizar umbrales de percentiles, análisis de anomalías de distribución, SVM y muchos otros métodos para la detección del desgaste de los componentes a lo largo del tiempo con la información que se proporciona aquí.**\n","\n","**Un problema con este método es la necesidad de preprocesar y escalar los datos por completo para luego entregarlos al algoritmo. Una forma de superar este problema es utilizar este conjunto de datos o una porción de él y combinarlo con una nueva porción de datos para los análisis del estado del sistema. Y para cada nuevo segmento de datos que necesite analizar, eliminamos el segmento agregado anterior y mantenemos este conjunto de datos intacto.** "]},{"cell_type":"markdown","metadata":{},"source":["# References\n","\n","**-- https://www.sciencedirect.com/science/article/pii/S221282711830307X**\n","\n","**-- https://towardsdatascience.com/machine-learning-for-anomaly-detection-and-condition-monitoring-d4614e7de770**\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}
